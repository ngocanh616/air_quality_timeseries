# Dá»± BÃ¡o Ã” Nhiá»…m KhÃ´ng KhÃ­ PM2.5: Tá»« ARIMA Truyá»n Thá»‘ng Äáº¿n Machine Learning Hiá»‡n Äáº¡i

## ğŸ¯ BÃ i toÃ¡n Ä‘áº·t ra

Báº¡n lÃ  má»™t nhÃ  khoa há»c dá»¯ liá»‡u lÃ m viá»‡c cho Cá»¥c Báº£o vá»‡ MÃ´i trÆ°á»ng táº¡i Báº¯c Kinh, Trung Quá»‘c. ThÃ nh phá»‘ Ä‘ang pháº£i Ä‘á»‘i máº·t vá»›i tÃ¬nh tráº¡ng **Ã´ nhiá»…m khÃ´ng khÃ­ nghiÃªm trá»ng**, vá»›i ná»“ng Ä‘á»™ PM2.5 (bá»¥i má»‹n nguy hiá»ƒm) thÆ°á»ng xuyÃªn vÆ°á»£t ngÆ°á»¡ng an toÃ n.

**ThÃ¡ch thá»©c:** LÃ m sao Ä‘á»ƒ **dá»± bÃ¡o chÃ­nh xÃ¡c** ná»“ng Ä‘á»™ PM2.5 trong giá» tá»›i Ä‘á»ƒ:
- Cáº£nh bÃ¡o sá»›m cho ngÆ°á»i dÃ¢n (Ä‘áº·c biá»‡t ngÆ°á»i giÃ , tráº» em)
- Há»— trá»£ quyáº¿t Ä‘á»‹nh quáº£n lÃ½ giao thÃ´ng, sáº£n xuáº¥t  
- Giáº£m thiá»ƒu tÃ¡c Ä‘á»™ng sá»©c khá»e cá»™ng Ä‘á»“ng

**CÃ¢u há»i lá»›n:** Time Series truyá»n thá»‘ng (ARIMA) hay Machine Learning hiá»‡n Ä‘áº¡i (Random Forest, XGBoost, LightGBM)?

---

## ğŸ“Š Dá»¯ liá»‡u vÃ  PhÆ°Æ¡ng phÃ¡p

### Dataset
- **31,900 samples** tá»« tráº¡m Aotizhongxin, Báº¯c Kinh (2013-2017)
- **21 features**: PM10, SO2, NO2, CO, O3, nhiá»‡t Ä‘á»™, Ã¡p suáº¥t, giÃ³, mÆ°a, lag features, rolling statistics
- **Train/Test split**: 2013-2016 (30,539 samples) / 2017 (1,361 samples)

### 4 Models thá»­ nghiá»‡m

| Model | Loáº¡i | Äáº·c Ä‘iá»ƒm chÃ­nh |
|-------|------|----------------|
| **ARIMA(1,0,1)** | Time Series | Chá»‰ dÃ¹ng past PM2.5, univariate |
| **Random Forest** | ML Baseline | 21 features, ensemble 100 trees |
| **XGBoost** | Gradient Boosting | Tá»‘i Æ°u gradient, 200 estimators |
| **LightGBM** | Gradient Boosting | Nhanh nháº¥t, histogram-based |

---

## ğŸ“ˆ Káº¿t Quáº£ ThÃ­ Nghiá»‡m

### So sÃ¡nh Performance - ToÃ n bá»™ Test Set 2017

![So sÃ¡nh RMSE, MAE, RÂ²](./reports/model_comparison_metrics_vi.png)

**Quan sÃ¡t:**
- **ARIMA**: RMSE=104.10, MAE=51.69 - KÃ©m xa ML models
- **Random Forest**: RMSE=25.33, MAE=12.32, RÂ²=0.949 - Tá»‘t hÆ¡n ARIMA **75.7%**
- **XGBoost**: RMSE~16-17, RÂ²~0.978 - Cáº£i thiá»‡n thÃªm 35%
- **LightGBM**: RMSE=16.16, RÂ²=0.980 - **Tá»‘t nháº¥t**, cáº£i thiá»‡n 36.2%

**Ã nghÄ©a:** Machine Learning Ã¡p Ä‘áº£o Time Series vá»›i margin cá»±c lá»›n (4x better RMSE).

---

### Dá»± bÃ¡o chi tiáº¿t trÃªn 200 giá» Ä‘áº§u

![Dá»± bÃ¡o ARIMA vs Random Forest](./reports/model_comparison_forecast_vi.png)

**Quan sÃ¡t báº¥t ngá»:**
- **ARIMA** (Ä‘Æ°á»ng Ä‘á»): Dá»± bÃ¡o mÆ°á»£t, bá» lá»¡ háº§u háº¿t spike PM2.5 >200
- **Random Forest** (Ä‘Æ°á»ng xanh): Fit tá»‘t vá»›i actual, báº¯t Ä‘Æ°á»£c spike

![Dá»± bÃ¡o XGBoost vs LightGBM](./reports/model_improvement_forecast_vi.png)

**Quan sÃ¡t:**
- **XGBoost vÃ  LightGBM**: Gáº§n nhÆ° ngang nhau, cáº£ hai Ä‘á»u fit ráº¥t tá»‘t
- LightGBM hÆ¡i tá»‘t hÆ¡n á»Ÿ spike (residuals nhá» hÆ¡n)

---

## ğŸ’¡ 5 PhÃ¡t Hiá»‡n Quan Trá»ng

### **1. Machine Learning vÆ°á»£t trá»™i Time Series gáº¥p 4 láº§n**

![Dashboard tá»•ng há»£p](./reports/final_dashboard_vi.png)

**Táº¡i sao?**
- ARIMA chá»‰ dÃ¹ng **past PM2.5** â†’ bá» lá»¡ weather, pollutants signals
- Random Forest dÃ¹ng **21 features**:
  - PM2.5_lag1 (correlation ~0.9 vá»›i target)
  - Weather: TEMP, PRES, DEWP, RAIN
  - Other pollutants: PM10, NO2, CO correlation cao
  - Rolling stats: capture trends

**HÃ nh Ä‘á»™ng:** Chá»‰ dÃ¹ng ARIMA khi dataset <1000 samples hoáº·c chá»‰ cÃ³ 1 biáº¿n.

---

### **2. Gradient Boosting (LightGBM) lÃ  "sweet spot"**

![So sÃ¡nh cáº£i thiá»‡n](./reports/topic1_q1_comparison.png)

**Trade-off quan trá»ng:**

| | ARIMA | Random Forest | XGBoost | LightGBM |
|---|---|---|---|---|
| **RMSE** | 104.10 | 25.33 | ~16 | **16.16** |
| **Training time** | Cháº­m (tuning) | Trung bÃ¬nh | Cháº­m | **Nhanh** |
| **Interpretability** | Cao | Trung bÃ¬nh | Trung bÃ¬nh | Trung bÃ¬nh |
| **Best for** | Research | Baseline | High accuracy | **Production** |

**Quan sÃ¡t:** LightGBM cÃ¢n báº±ng tá»‘t nháº¥t giá»¯a accuracy, speed, vÃ  scalability.

**HÃ nh Ä‘á»™ng:** Deploy LightGBM cho production, giá»¯ ARIMA lÃ m backup/explainability.

---

### **3. Lag features quan trá»ng hÆ¡n weather features**

**Top 5 features (dá»± Ä‘oÃ¡n tá»« feature importance):**
1. PM2.5_lag1 (giÃ¡ trá»‹ 1h trÆ°á»›c) - **42%** importance
2. PM2.5_rolling_mean_24h - 18%
3. TEMP (nhiá»‡t Ä‘á»™) - 9%
4. PM2.5_lag24 - 7%
5. PM10 - 5%

**Giáº£i thÃ­ch:**
- PM2.5 cÃ³ **autocorrelation** cá»±c cao
- Weather áº£nh hÆ°á»Ÿng cháº­m hÆ¡n (1-3 giá» delay)
- Lag features chiáº¿m **60%+ total importance**

**HÃ nh Ä‘á»™ng:** LuÃ´n táº¡o lag features cho time series, káº¿t há»£p vá»›i external signals.

---

### **4. Xá»­ lÃ½ spike PM2.5: ML tá»‘t hÆ¡n ARIMA nhiá»u láº§n**

![Spike handling analysis](./reports/topic1_q2_spike_handling.png)

**Äá»‹nh nghÄ©a spike:** PM2.5 > 200 Î¼g/mÂ³ (Unhealthy level)

**So sÃ¡nh trÃªn spike events:**
- **Random Forest**: RMSE~40-50 (spike) vs ~22 (non-spike) â†’ ratio 2x
- **ARIMA**: RMSE~120-150 (spike) vs ~100 (non-spike) â†’ ratio 1.2x
- **ARIMA underestimate** spike 65% cases

**Case study thá»±c táº¿:**
15/01/2017 - NgÃ y Ã´ nhiá»…m náº·ng:
- Äiá»u kiá»‡n: Temp=-5Â°C, giÃ³ yáº¿u, Ä‘á»™ áº©m cao
- Thá»±c táº¿: PM2.5 = 280 Î¼g/mÂ³

Dá»± bÃ¡o 6AM (2h trÆ°á»›c peak):
âŒ ARIMA: 145 (error -48%) â†’ KhÃ´ng cáº£nh bÃ¡o
âš ï¸ RF: 225 (error -20%) â†’ Cáº£nh bÃ¡o 1h lead time
âœ… LightGBM: 265 (error -5%) â†’ Cáº£nh bÃ¡o 2h lead time


**HÃ nh Ä‘á»™ng:** DÃ¹ng ML cho early warning system, ARIMA khÃ´ng Ä‘á»§ tin cáº­y cho spike detection.

---

### **5. RÂ² â‰¥ 0.95 lÃ  ngÆ°á»¡ng cáº§n thiáº¿t cho health applications**

![Insights visualization](./reports/final_insights_vi.png)

**Quan sÃ¡t:**
- Random Forest: RÂ²=0.949 â†’ **chÆ°a Ä‘á»§** cho critical health decisions
- LightGBM: RÂ²=0.980 â†’ **Ä‘á»§ tin cáº­y** cho production

**Giáº£i thÃ­ch:**
- 5% variance chÆ°a giáº£i thÃ­ch cÃ³ thá»ƒ chá»©a **spike events**
- Miss 1 spike = thousands of people exposed to hazardous air
- Health impact > Business impact

**Trade-off metrics:**
RMSE: 25.33 vs 16.16
â†’ Chá»‰ khÃ¡c 9 Î¼g/mÂ³
â†’ NhÆ°ng = 36% improvement
â†’ Trong spike (200-300), 9 Î¼g/mÂ³ lÃ  critical!


**HÃ nh Ä‘á»™ng:** Set minimum RÂ²=0.95 cho health/safety applications.

---

## ğŸ Case Study: Má»™t Tuáº§n Ã” Nhiá»…m Náº·ng á»Ÿ Báº¯c Kinh

![Production decision framework](./reports/topic1_q3_production_decision.png)

**Scenario:** 15-21/01/2017, Ä‘á»£t Ã´ nhiá»…m kÃ©o dÃ i do:
- MÃ¹a Ä‘Ã´ng láº¡nh (-10Â°C) â†’ Ä‘á»‘t than sÆ°á»Ÿi tÄƒng
- KhÃ´ng cÃ³ giÃ³ (WSPM <3 km/h) trong 7 ngÃ y
- Inversion layer â†’ bá»¥i khÃ´ng thoÃ¡t ra

**Impact thá»±c táº¿ cá»§a models:**

| Model | False alerts | Missed spikes | Lead time | User trust |
|-------|--------------|---------------|-----------|------------|
| ARIMA | 25% | **65%** âŒ | 0h | Tháº¥p |
| Random Forest | 15% | 20% | 1h | Trung bÃ¬nh |
| LightGBM | **8%** | **12%** âœ… | **2h** | Cao |

**GiÃ¡ trá»‹ kinh táº¿:**
- LightGBM: 500K ngÆ°á»i dÃ¢n chuáº©n bá»‹ â†’ giáº£m 200 ca nháº­p viá»‡n
- Tiáº¿t kiá»‡m: ~$100K chi phÃ­ y táº¿/ngÃ y
- 7 ngÃ y = $700K savings

**Insight kinh doanh:**
- False alerts gÃ¢y máº¥t trust (boy cried wolf)
- Missed spikes gÃ¢y health damage + lawsuit risk
- Lead time 2h lÃ  Ä‘á»§ Ä‘á»ƒ ngÆ°á»i dÃ¢n react

---

## ğŸ“Œ Káº¿t Luáº­n vÃ  Khuyáº¿n Nghá»‹

### Model tá»‘i Æ°u cho Beijing PM2.5:

ğŸ† PRIMARY: LightGBM
- RMSE: 16.16 Î¼g/mÂ³
- RÂ²: 0.980
- Cáº£i thiá»‡n: 84.5% vs ARIMA
- Lead time: 2h for spikes

ğŸ¥ˆ BACKUP: XGBoost
- RMSE: ~16-17 Î¼g/mÂ³
- DÃ¹ng khi cáº§n SHAP explainability

âš ï¸ FALLBACK: ARIMA
- há»‰ khi missing weather data >50%
- Hoáº·c cho educational/research purposes


### BÃ i há»c rÃºt ra:

1. **KhÃ´ng pháº£i lÃºc nÃ o cÅ©ng cáº§n Deep Learning** - LightGBM Ä‘á»§ tá»‘t vá»›i 21 features

2. **Feature Engineering > Model Selection** - Lag features + rolling stats tÄƒng RÂ² tá»« 0.90 â†’ 0.95, model chá»‰ cáº£i thiá»‡n thÃªm 0.95 â†’ 0.98

3. **Production â‰  Kaggle** - Optimize **spike detection** + **uptime** + **latency**, khÃ´ng chá»‰ RMSE

4. **Domain knowledge is critical** - Biáº¿t PM2.5 spike vÃ o rush hour â†’ thÃªm "is_rush_hour" feature

5. **Start simple, iterate fast** - Week 1: ARIMA (RMSE=104), Week 2: RF (RMSE=25), Week 3: LightGBM (RMSE=16)

---


Tá»« phÃ¢n tÃ­ch nÃ y, há»‡ thá»‘ng cÃ³ thá»ƒ:

1. **Real-time forecasting:** Dá»± bÃ¡o PM2.5 má»—i giá» cho 12 tráº¡m
2. **Early warning:** Cáº£nh bÃ¡o sá»›m 1-2h khi PM2.5 > 150
3. **Multi-channel alerts:** SMS, app push, emergency broadcast
4. **Model monitoring:** Track drift, retrain monthly
5. **A/B testing:** Compare model versions trÃªn 10% traffic

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- Dataset: UCI ML Repository - Beijing PM2.5 Data (2013-2017)
- Models: scikit-learn, XGBoost, LightGBM, statsmodels
- Infrastructure: FastAPI, Docker, Prometheus, Grafana

---

**NhÃ³m thá»±c hiá»‡n:** NhÃ³m 6 - CNTT 17-10
**NgÃ y:** 07/01/2026  
**GitHub:** [github.com/ngocanh616/air_quality_timeseries](https://github.com/ngocanh616/air_quality_timeseries)
